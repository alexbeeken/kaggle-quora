{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs Kaggle Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the csv\n",
    "\n",
    "Import CSV and store selected rows in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "questions = []\n",
    "\n",
    "with open('q_quora_100.csv', 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        questions.append(row[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first row of the csv, the column keys. For double checking that the columns are the ones we want to easily refer back to see which columns are which."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names\n",
      "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n"
     ]
    }
   ],
   "source": [
    "keys = questions[0]\n",
    "print 'column names'\n",
    "print keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify any columns that have the wrong number of columns. This was found because commas were used in some questions which messed up the csv parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in questions:\n",
    "    if len(row) != 6:\n",
    "        print 'WARNING: A COLUMN NEEDS FIXING' + str(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert python list to numpy array and delete the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '2'\n",
      " 'What is the step by step guide to invest in share market in india?'\n",
      " 'What is the step by step guide to invest in share market?' '0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "questions = np.array(questions)\n",
    "questions = np.delete(questions, 0, 0)\n",
    "print questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "First take a random entry in the dataset to get a feel for the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>6</td><td>13</td><td>14</td><td>Should I buy tiago?</td><td>What keeps childern active and far from phone and video games?</td><td>0</td></tr><tr><td>6</td><td>13</td><td>14</td><td>Should I buy tiago?</td><td>What keeps childern active and far from phone and video games?</td><td>0</td></tr><tr><td>6</td><td>13</td><td>14</td><td>Should I buy tiago?</td><td>What keeps childern active and far from phone and video games?</td><td>0</td></tr><tr><td>6</td><td>13</td><td>14</td><td>Should I buy tiago?</td><td>What keeps childern active and far from phone and video games?</td><td>0</td></tr><tr><td>6</td><td>13</td><td>14</td><td>Should I buy tiago?</td><td>What keeps childern active and far from phone and video games?</td><td>0</td></tr><tr><td>6</td><td>13</td><td>14</td><td>Should I buy tiago?</td><td>What keeps childern active and far from phone and video games?</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "size = len(questions)\n",
    "random_index = random.randrange(size)\n",
    "random_question = questions[random_index]\n",
    "\n",
    "\n",
    "display(HTML(\n",
    "    '<table><tr>{}</tr></table>'.format(\n",
    "        '</tr><tr>'.join(\n",
    "            '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in random_question)) for row in random_question)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a count of how many duplicates and what percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates: 36 percentage: 0.349514563107%\n",
      "not duplicates: 67 percentage: 0.650485436893%\n"
     ]
    }
   ],
   "source": [
    "duplicates = 0\n",
    "not_duplicates = 0\n",
    "total = len(questions)\n",
    "\n",
    "for row in questions:\n",
    "    if row[5] == '0':\n",
    "        not_duplicates += 1\n",
    "    else:\n",
    "        duplicates += 1\n",
    "\n",
    "print 'duplicates: ' + str(duplicates) + ' percentage: ' + str((float(duplicates) / float(total))) + '%'\n",
    "print 'not duplicates: ' + str(not_duplicates) + ' percentage: ' + str(float(not_duplicates) / float(total)) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "### Munging\n",
    "\n",
    "Downcase and then split the sentence into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india?'], ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market?']], ['1', ['what', 'is', 'the', 'story', 'of', 'kohinoor', '(koh-i-noor)', 'diamond?'], ['what', 'would', 'happen', 'if', 'the', 'indian', 'government', 'stole', 'the', 'kohinoor', '(koh-i-noor)', 'diamond', 'back?']], ['2', ['how', 'can', 'i', 'increase', 'the', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'a', 'vpn?'], ['how', 'can', 'internet', 'speed', 'be', 'increased', 'by', 'hacking', 'through', 'dns?']], ['3', ['why', 'am', 'i', 'mentally', 'very', 'lonely?', 'how', 'can', 'i', 'solve', 'it?'], ['find', 'the', 'remainder', 'when', '[math]23^{24}[/math]', 'is', 'divided', 'by', '24,23?']]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "processing = []\n",
    "\n",
    "for data in questions:\n",
    "    pairId = data[0]\n",
    "    sentence1 = data[3].lower()\n",
    "    sentence2 = data[4].lower()\n",
    "    tokens1 = sentence1.split(' ')\n",
    "    tokens2 = sentence2.split(' ')\n",
    "    processing.append([\n",
    "        pairId,\n",
    "        tokens1,\n",
    "        tokens2\n",
    "    ])\n",
    "    \n",
    "print(processing[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', ['step', 'step', 'guide', 'invest', 'share', 'market', 'india'], ['step', 'step', 'guide', 'invest', 'share', 'market']], ['1', ['story', 'kohinoor', 'kohinoor', 'diamond'], ['would', 'happen', 'indian', 'government', 'stole', 'kohinoor', 'kohinoor', 'diamond', 'back']], ['2', ['increase', 'speed', 'internet', 'connection', 'using', 'vpn'], ['internet', 'speed', 'increased', 'hacking', 'dns']], ['3', ['mentally', 'lonely', 'solve', 'it'], ['find', 'remainder', 'mathmath', 'divided']], ['4', ['one', 'dissolve', 'water', 'quikly', 'sugar', 'salt', 'methane', 'carbon', 'di', 'oxide'], ['fish', 'would', 'survive', 'salt', 'water']]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexbeeken/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "munged = []\n",
    "\n",
    "def remove_stop(sentence, words=stopwords.words('english')):\n",
    "    remove_these = []\n",
    "    for i in range(0, len(sentence) - 1):\n",
    "        word = sentence[i]\n",
    "        if word in words:\n",
    "            remove_these.append(i)\n",
    "    output = []\n",
    "    for i in range(0, len(sentence)):\n",
    "        if i not in remove_these:\n",
    "            punctuationless = remove_punctuation(sentence[i])\n",
    "            if len(punctuationless) > 0:\n",
    "                output.append(punctuationless)\n",
    "    return output\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    return re.sub('[^a-zA-Z]', '', word)\n",
    "\n",
    "for data in processing:\n",
    "    pairId = data[0]\n",
    "    sentence1 = data[1]\n",
    "    sentence2 = data[2]\n",
    "    out1 = remove_stop(sentence1)\n",
    "    out2 = remove_stop(sentence2)\n",
    "    munged.append([\n",
    "        pairId,\n",
    "        out1,\n",
    "        out2\n",
    "    ])\n",
    "    \n",
    "print munged[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Order words by popularity\n",
    "\n",
    "We'll create a list of all words used in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step', 'step', 'guide', 'invest', 'share', 'market', 'india', 'step', 'step', 'guide', 'invest', 'share', 'market', 'story', 'kohinoor', 'kohinoor', 'diamond', 'would', 'happen', 'indian']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "\n",
    "for row in munged:\n",
    "    for word in row[1]:\n",
    "        all_tokens.append(word)\n",
    "    for word in row[2]:\n",
    "        all_tokens.append(word)\n",
    "\n",
    "print all_tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll count the occurences of each word and order it by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('best', 15), ('make', 12), ('learn', 8), ('much', 8), ('like', 8), ('would', 7), ('find', 6), ('ever', 6), ('india', 6), ('someone', 6), ('book', 6), ('quora', 5), ('phone', 5), ('me', 5), ('good', 5), ('effects', 5), ('increase', 5), ('get', 5), ('ask', 5), ('one', 5)]\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_MAX = 1000\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "count = Counter(all_tokens)\n",
    "top_count = count.most_common(VOCABULARY_MAX)\n",
    "print top_count[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make the vocabulary count into a usable list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 words: ['best', 'make', 'learn', 'much', 'like', 'would', 'find', 'ever', 'india', 'someone', 'book', 'quora', 'phone', 'me', 'good', 'effects', 'increase', 'get', 'ask', 'one']\n",
      "\n",
      "actual vocabulary size: 676\n",
      "\n",
      "percentage of original corpus: 100.0%\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "\n",
    "for tup in top_count:\n",
    "    vocabulary.append(tup[0])\n",
    "\n",
    "print 'top 20 words: ' + str(vocabulary[0:20])\n",
    "print '\\nactual vocabulary size: ' + str(len(vocabulary))\n",
    "print '\\npercentage of original corpus: '+ str(float(len(vocabulary)) / float(len(count)) * 100) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the actual strings in our data to indexes in the vocabulary list. Examine how many words are left out of our vocabulary with our vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: \n",
      "['0', [43, 43, 217, 283, 124, 110, 8], [43, 43, 217, 283, 124, 110]]\n",
      "\n",
      "total none count: 15\n",
      "\n",
      "percentage of corpus removed: 1.30548302872%\n"
     ]
    }
   ],
   "source": [
    "indexes = []\n",
    "noneCount = 0\n",
    "\n",
    "def vocabIndex(token):\n",
    "    try:\n",
    "        return vocabulary.index(token)\n",
    "    except ValueError:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def indexify(tokens):\n",
    "    global noneCount\n",
    "    output = []\n",
    "    for token in tokens:\n",
    "        index = vocabIndex(token)\n",
    "        if index:\n",
    "            output.append(index)\n",
    "        else:\n",
    "            noneCount += 1\n",
    "    return output\n",
    "        \n",
    "for row in munged:\n",
    "    indexes.append([\n",
    "        row[0],\n",
    "        indexify(row[1]),\n",
    "        indexify(row[2])\n",
    "    ])\n",
    "\n",
    "print 'example: '\n",
    "print indexes[0]\n",
    "print '\\ntotal none count: ' + str(noneCount)\n",
    "print '\\npercentage of corpus removed: ' + str(float(noneCount) / float(len(all_tokens)) * 100) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the dataset into a Training and Testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98, 120, 201, 128, 123, 502, 98, 120, 201, 128, 123, 415, 412]\n",
      "0\n",
      "length of x training set: 78\n",
      "length of x testing set: 25\n",
      "length of y training set: 78\n",
      "length of y testing set: 25\n"
     ]
    }
   ],
   "source": [
    "xTrain = []\n",
    "yTrain = []\n",
    "xTest = []\n",
    "yTest = []\n",
    "    \n",
    "for index in indexes:\n",
    "    if int(index[0]) < (len(indexes) / 4):\n",
    "        xTest.append(index[1] + index[2])\n",
    "    else:\n",
    "        xTrain.append(index[1] + index[2])\n",
    "        \n",
    "for question in questions:\n",
    "    if int(question[0]) < (len(questions) / 4):\n",
    "        yTest.append(int(question[5]))\n",
    "    else:\n",
    "        yTrain.append(int(question[5]))\n",
    "        \n",
    "print xTrain[0]\n",
    "print yTrain[0]\n",
    "print 'length of x training set: ' + str(len(xTrain))\n",
    "print 'length of x testing set: ' + str(len(xTest))\n",
    "print 'length of y training set: ' + str(len(yTrain))\n",
    "print 'length of y testing set: ' + str(len(yTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate our new average size of combined indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length of training set: 11.1666666667\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "\n",
    "for row in xTrain:\n",
    "    total += len(row)\n",
    "\n",
    "print 'average length of training set: ' + str(float(total)/float(len(xTrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal length for responses we'll plot the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFkCAYAAADFZ4k9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAE5hJREFUeJzt3W+MZXd93/HPlxi6HtesFVDXWxFhHJvs7FqhzKS4LjIg\nOYoBqRtXitJOWVGKCLIoElqpEoqC6gVLjWRE12oSSwhVBGuTkRxFxCbC3lBCUiwKVDMmidczMRC7\n/LG9drE0Rl5vgPjXB3fs7i7r2bkz9+6d2d/rJV0d33PPnfN9sn7POefeM9VaCwBwYXvZpAcAAMZP\n8AGgA4IPAB0QfADogOADQAcEHwA6IPgA0AHBB4AOCD4AdEDwAaADQwW/qm6uqr+qqpXVx1eq6u1n\nbPOxqnqsqk5U1Req6qrRjgwADGvYI/zvJvlwkpkks0n+PMndVTWdJFX14SQfTPL+JG9K8mySo1X1\nipFNDAAMrTb7x3Oq6gdJ/lNr7dNV9ViSj7fWDq++9sokx5P8+9baXZueFgDYkA1fw6+ql1XVv00y\nleQrVfW6JJcn+eIL27TWnknytSTXbXZQAGDjLhr2DVV1TZL/lWRHkh8m+dettb+tquuStAyO6E91\nPINfBF7q570qyY1JHk1ycth5AKBjO5JckeRoa+0Ha204dPCTLCd5Q5KdSX4tyZ1V9ZYN/JwX3Jjk\nDzbxfgDo3buS/OFaGwwd/NbaT5L83erTB6rqTUk+lOS2JJVkV04/yt+V5IE1fuSjSXLkyJFMT08P\nOw5wHhw8eDCHDx+e9BjAGZaWlnLgwIFktaVr2cgR/pleluQftdYeqaonktyQ5K+TFz+0d22S31vj\n/SeTZHp6OjMzMyMYBxi1nTt3+vcJW9s5L4kPFfyq+i9J7k3ynSSXZnAK4a1JfmV1k9uTfKSqvpXB\nbxu3JvlekruH2Q8AMFrDHuH/kySfSbI7yUoGR/K/0lr78yRprd1WVVNJPpnksiRfTvKO1tqPRjcy\nADCsoYLfWnvfOrY5lOTQBucBAMbAvfSBc5qbm5v0CMAmCT5wToIP25/gA0AHBB8AOiD4ANABwQeA\nDgg+AHRA8AGgA4IPAB0QfADogOADQAcEHwA6IPgA0AHBB4AOCD4AdEDwAaADgg8AHRB8AOiA4ANA\nBwQfADog+ADQAcEHzml+fn7SIwCbJPjAOQk+bH+CDwAdEHwA6MBFkx4A2Hrm5+dPO43/uc99Lvv3\n73/x+dzcXObm5iYxGrBBgg/8lDODvn///txzzz0TnAjYLKf0AaADgg8AHRB84Jxcr4ftT/CBcxJ8\n2P4EHwA6IPjAObnTHmx/gg+ck+DD9if4ANABwQeADrjTHvBT3FoXLjyCD/wUt9aFC49T+gDQAcEH\ngA4IPnBOrtfD9jdU8KvqN6vq61X1TFUdr6rPVtXrz9jm01X1/BmPz492bOB8EnzY/oY9wr8+ye8k\nuTbJLyd5eZI/q6qLz9ju3iS7kly++vB/CwCYoKE+pd9ae+epz6vqPUmeTDKb5P5TXvr71tpTm54O\nABiJzV7DvyxJS/L0GevftnrKf7mq7qiqn93kfgCATdjw9/CrqpLcnuT+1tpDp7x0b5I/TvJIkp9P\n8ttJPl9V17XW2maGBQA2ZjM33rkjyd4kbz51ZWvtrlOeHquqv0ny7SRvS/Kll/phBw8ezM6dO09b\n525eADBw5h0wk2RlZWXd76+NHHRX1e8m+VdJrm+tfWcd2z+Z5Ldaa586y2szSRYWFhYyMzMz9CwA\n0KvFxcXMzs4myWxrbXGtbYc+wl+N/a8mees6Y/+aJK9K8viw+wIARmPY7+HfkeRdSf5dkmeratfq\nY8fq65dU1W1VdW1VvbaqbkjyJ0keTnJ01MMDAOsz7Kf0b07yyiR/keSxUx6/vvr6PyT5xSR3J/nb\nJJ9K8r+TvKW19uMRzAsAbMCw38Nf8xeE1trJJG/f1EQAwMi5lz4AdEDwAaADgg8AHdjMjXeALezE\niRNZXl6e9Bin2bNnT6ampiY9BnRJ8OECtby8/MINObYMN9iCyRF8uEDt2bMnCwsLm/45S0vJgQPJ\nkSPJ9PTmZwImQ/DhAjU1NTXSo+np6cTBOWxfPrQHAB0QfGBNu3cnt9wyWALbl1P6wJp2704OHZr0\nFMBmOcIHgA4IPgB0QPABoAOCDwAdEHwA6IDgA0AHBB9Y03PPJceODZbA9iX4wJqWlpJrrhksge1L\n8AGgA4IPAB0QfADogOADQAcEHwA6IPgA0AHBB4AOXDTpAYCtbXo6efDB5MorJz0JsBmCD6zp4ouT\nffsmPQWwWU7pA0AHBB8AOiD4ANABwQeADgg+AHRA8AGgA4IPrOnxx5NDhwZLYPsSfGBNjz+efPSj\ngg/bneADQAcEHwA6IPgA0AHBB4AOCD4AdGCo4FfVb1bV16vqmao6XlWfrarXn2W7j1XVY1V1oqq+\nUFVXjW5kAGBYwx7hX5/kd5Jcm+SXk7w8yZ9V1cUvbFBVH07ywSTvT/KmJM8mOVpVrxjJxMB5tWNH\nsnfvYAlsXxcNs3Fr7Z2nPq+q9yR5MslskvtXV38oya2ttT9d3ebdSY4nuSnJXZucFzjP9u5Njh2b\n9BTAZm32Gv5lSVqSp5Okql6X5PIkX3xhg9baM0m+luS6Te4LANigDQe/qirJ7Unub609tLr68gx+\nATh+xubHV18DACZgqFP6Z7gjyd4kbx7RLADAmGwo+FX1u0nemeT61tqpd9h+Ikkl2ZXTj/J3JXlg\nrZ958ODB7Ny587R1c3NzmZub28iIAHBBmZ+fz/z8/GnrVlZW1v3+aq0NtcPV2P9qkre21v7uLK8/\nluTjrbXDq89fmUH8391a+6OzbD+TZGFhYSEzMzNDzQIAPVtcXMzs7GySzLbWFtfadqgj/Kq6I8lc\nkv1Jnq2qXasvrbTWTq7+9+1JPlJV30ryaJJbk3wvyd3D7AsAGJ1hT+nfnMGH8v7ijPX/IcmdSdJa\nu62qppJ8MoNP8X85yTtaaz/a3KgAwEYN9Sn91trLWms/c5bHnWdsd6i19k9ba1OttRtba98a7djA\n+fLQQ8m+fYMlsH25lz6wppMnB7E/efLc2wJbl+ADQAcEHwA6IPgA0AHBB4AOCD4AdEDwAaADgg+s\naffu5JZbBktg+9rMX8sDOrB7d3Lo0KSnADbLET4AdEDwAaADgg8AHRB8AOiA4ANABwQfADog+MCa\nnnsuOXZssAS2L8EH1rS0lFxzzWAJbF+CDwAdEHwA6IDgA0AHBB8AOiD4ANABwQeADgg+AHTgokkP\nAGxt09PJgw8mV1456UmAzRB8YE0XX5zs2zfpKYDNckofADog+ADQAcEHgA4IPgB0QPABoAOCDwAd\nEHxgTY8/nhw6NFgC25fgA2t6/PHkox8VfNjuBB8AOiD4ANABwQeADgg+AHRA8AGgA4IPAB0QfGBN\nO3Yke/cOlsD2NXTwq+r6qrqnqr5fVc9X1f4zXv/06vpTH58f3cjA+bR3b3Ls2GAJbF8bOcK/JMk3\nknwgSXuJbe5NsivJ5auPuQ1NBwCMxEXDvqG1dl+S+5KkquolNvv71tpTmxkMABidcV3Df1tVHa+q\n5aq6o6p+dkz7AQDWYegj/HW4N8kfJ3kkyc8n+e0kn6+q61prL3UJAAAYo5EHv7V21ylPj1XV3yT5\ndpK3JfnSS73v4MGD2blz52nr5ubmMjfn8j8AzM/PZ35+/rR1Kysr635/beagu6qeT3JTa+2ec2z3\nZJLfaq196iyvzSRZWFhYyMzMzIZnAYDeLC4uZnZ2NklmW2uLa2079u/hV9VrkrwqiT+uCQATspHv\n4V9SVW+oqn+2uurK1ec/t/rabVV1bVW9tqpuSPInSR5OcnSUgwPnx0MPJfv2DZbA9rWRa/i/lMG1\n+Lb6+MTq+s9k8N38X0zy7iSXJXksg9D/59bajzc9LXDenTw5iP3Jk5OeBNiMjXwP/y+z9pmBt298\nHABgHNxLHwA6IPgA0AHBB4AOCD4AdGAct9YFRuCb30x++MNJT5EsLZ2+nLRLL02uvnrSU8D2I/iw\nBX3zm8nrXz/pKU534MCkJ/j/Hn5Y9GFYgg9b0AtH9keOJNPTk51lK1laGvzisRXOfMB2I/iwhU1P\nJ/7EBDAKPrQHAB0QfADogOADQAcEHwA6IPgA0AHBB4AOCD4AdEDwAaADgg8AHRB8AOiA4ANABwQf\nADog+ADQAcEHgA4IPgB0QPABoAOCDwAdEHwA6IDgA0AHBB8AOiD4ANABwQeADgg+AHRA8AGgA4IP\nAB0QfADogOADQAcEHwA6IPgA0AHBB4AOCD4AdEDwAaADgg8AHRg6+FV1fVXdU1Xfr6rnq2r/Wbb5\nWFU9VlUnquoLVXXVaMYFADZiI0f4lyT5RpIPJGlnvlhVH07ywSTvT/KmJM8mOVpVr9jEnADAJlw0\n7Btaa/cluS9JqqrOssmHktzaWvvT1W3eneR4kpuS3LXxUQGAjRrpNfyqel2Sy5N88YV1rbVnknwt\nyXWj3BcAsH5DH+Gfw+UZnOY/fsb646uvAetQz53IG7Oci5cmPcnWcvFS8sYk9dyeJFOTHge2lVEH\nf8MOHjyYnTt3nrZubm4uc3NzE5oIJmfHo8tZzGxyYNKTbC3TSRaTLD26kLx5ZtLjwHk1Pz+f+fn5\n09atrKys+/2jDv4TSSrJrpx+lL8ryQNrvfHw4cOZmfEPGJLk5BV7MpOF/MGRZHp60tNsHUtLybsO\nJP/9ij2THgXOu7MdBC8uLmZ2dnZd7x9p8Ftrj1TVE0luSPLXSVJVr0xybZLfG+W+4ELWLp7KA5nJ\nc9NJ/B78oucyOHJoF096Eth+hg5+VV2S5KoMjuST5MqqekOSp1tr301ye5KPVNW3kjya5NYk30ty\n90gmBgCGtpEj/F9K8qUMPpzXknxidf1nkry3tXZbVU0l+WSSy5J8Ock7Wms/GsG8AMAGbOR7+H+Z\nc3ydr7V2KMmhjY0EAIyae+kDQAcEHwA6IPgA0AHBB4AOCD4AdEDwAaADgg8AHRB8AOiA4ANABwQf\nADog+ADQAcEHgA4IPgB0QPABoAOCDwAdEHwA6IDgA0AHBB8AOiD4ANABwQeADgg+AHRA8AGgA4IP\nAB0QfADowEWTHgD4aSdODJaLi5OdY6tZWpr0BLB9CT5sQcvLg+Vv/MZk59iqLr100hPA9iP4sAXd\ndNNguWdPMjU12VmWlpIDB5IjR5Lp6cnOkgxif/XVk54Cth/Bhy3o1a9O3ve+SU9xuunpZGZm0lMA\nG+VDewDQAcEHgA4IPgB0QPABoAOCDwAdEHxgTTt2JHv3DpbA9uVrecCa9u5Njh2b9BTAZjnCB4AO\nCD4AdEDwAaADgg8AHRB8AOjAyINfVbdU1fNnPB4a9X4AgPUb19fyHkxyQ5Jaff6TMe0HAFiHcZ3S\n/0lr7anW2pOrj6fHtB9gzB56KNm3b7AEtq9xBf/qqvp+VX27qo5U1c+NaT/AmJ08OYj9yZOTngTY\njHEE/6tJ3pPkxiQ3J3ldkv9ZVZeMYV8AwDqM/Bp+a+3oKU8frKqvJ/k/SX49yadHvT8A4NzGfi/9\n1tpKVT2c5Kq1tjt48GB27tx52rq5ubnMzc2NczwA2Bbm5+czPz9/2rqVlZV1v3/swa+qf5xB7O9c\na7vDhw9nZmZm3OMAwLZ0toPgxcXFzM7Oruv94/ge/ser6i1V9dqq+pdJPpvkx0nmz/FWAGBMxnGE\n/5okf5jkVUmeSnJ/kn/RWvvBGPYFAKzDOD6056I7XEB2705uuWWwBLavsV/DB7a33buTQ4cmPQWw\nWf54DgB0QPABoAOCDwAdcA0fLlAnTpzI8vLypMc4zZ49ezI1NTXpMaBLgg8XqOXl5XXfkON8WVhY\ncIMtmBDBhwvUnj17srCwMOkxTrNnz55JjwDdEny4QE1NTTmaBl7kQ3sA0AHBB4AOCD4AdEDwAaAD\ngg8AHRB8AOiA4ANABwQfADog+ADQAcEHgA4IPgB0QPABoAOCDwAdEHwA6IDgA0AHBB8AOiD4ANAB\nwQeADgg+AHRA8AGgA4IPAB0QfADogOADQAcEHwA6IPgA0AHBB4AOCD4AdEDwAaADgg8AHRB8AOiA\n4ANABwQfADog+MA5zc/PT3oEYJPGFvyq+o9V9UhVPVdVX62qfz6ufQHjJfiw/Y0l+FX1b5J8Iskt\nSd6Y5K+SHK2qV49jfwDA2sZ1hH8wySdba3e21paT3JzkRJL3jml/AMAaRh78qnp5ktkkX3xhXWut\nJfkfSa4b9f4AgHO7aAw/89VJfibJ8TPWH0/yC2fZfkeSLC0tjWEUYBRWVlayuLg46TGAM5zSzh3n\n2nYcwR/WFUly4MCBCY8BrGV2dnbSIwAv7YokX1lrg3EE//8m+Ycku85YvyvJE2fZ/miSdyV5NMnJ\nMcwDABeqHRnE/ui5NqzB5fXRqqqvJvlaa+1Dq88ryXeS/LfW2sdHvkMAYE3jOqX/X5P8flUtJPl6\nBp/an0ry+2PaHwCwhrEEv7V21+p37j+Wwan8byS5sbX21Dj2BwCsbSyn9AGArcW99AGgA4IPAB0Q\nfOCsqur6qrqnqr5fVc9X1f5JzwRsnOADL+WSDD5w+4EkPuwD29xWuNMesAW11u5Lcl/y4r00gG3M\nET4AdEDwAaADgg8AHRB8AOiA4ANAB3xKHzirqrokyVVJXviE/pVV9YYkT7fWvju5yYCNcC994Kyq\n6q1JvpSf/g7+Z1pr753ASMAmCD4AdMA1fADogOADQAcEHwA6IPgA0AHBB4AOCD4AdEDwAaADgg8A\nHRB8AOiA4ANABwQfADrw/wCwO9kqnFspKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118708bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "result = [len(x) for x in xTrain]\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a good number to capture most of the data is 20 so we'll set a constant to define that. I'll put a few other hyper parameters in here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 20\n",
    "OUTPUT_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0  98 120 201 128 123 502  98 120 201 128 123\n",
      "  415 412]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 235 278 235\n",
      "  278 392]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 146 429   3 522 263 263\n",
      "  657 146]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  76   1  41  36  76  18\n",
      "   41  36]]\n",
      "[[  0   0   0   0   0   0   0  43  43 217 283 124 110   8  43  43 217 283\n",
      "  124 110]\n",
      " [  0   0   0   0   0   0   0 437  34  34 113   5 404  97  71 364  34  34\n",
      "  113  73]\n",
      " [  0   0   0   0   0   0   0   0   0  16  23  33 529  72 349  33  23 594\n",
      "  453 343]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 569 640 577 558   6 422\n",
      "  458 379]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(xTrain, maxlen=SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(xTest, maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "print X_train[0:4]\n",
    "print X_test[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm concatenating the training and testing data into single lists. This was by mistake but I might need it later so I'm keeping it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   44,\n",
       "   209,\n",
       "   163,\n",
       "   373,\n",
       "   19,\n",
       "   243,\n",
       "   449,\n",
       "   420,\n",
       "   44,\n",
       "   209,\n",
       "   163,\n",
       "   520],\n",
       "  1]]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = []\n",
    "test = []\n",
    "\n",
    "for num in range(len(X_train)):\n",
    "    train.append([list(X_train[num]), yTrain[num]])\n",
    "\n",
    "for num in range(len(X_test)):\n",
    "    test.append([list(X_test[num]), yTest[num]])\n",
    "\n",
    "train[4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 20, 32)            21632     \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 250)               160250    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 182,133.0\n",
      "Trainable params: 182,133.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCABULARY_SIZE, OUTPUT_DIM, input_length=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert X_train and Y_train to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 20)\n",
      "(25, 20)\n",
      "(78,)\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X_train)\n",
    "Xt = np.array(X_test)\n",
    "Y = np.array(yTrain)\n",
    "Yt = np.array(yTest)\n",
    "\n",
    "print X.shape\n",
    "print Xt.shape\n",
    "print Y.shape\n",
    "print Yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78 samples, validate on 25 samples\n",
      "Epoch 1/2\n",
      "0s - loss: 0.6953 - acc: 0.4231 - val_loss: 0.6759 - val_acc: 0.6400\n",
      "Epoch 2/2\n",
      "0s - loss: 0.6624 - acc: 0.6795 - val_loss: 0.6642 - val_acc: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b49ac10>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, validation_data=(Xt, Yt), epochs=2, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66420888900756836, 0.63999998569488525]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(Xt, Yt, verbose=0)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
