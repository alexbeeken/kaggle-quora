{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Questions Kaggle Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the csv\n",
    "\n",
    "Import CSV and store selected rows in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "questions = []\n",
    "\n",
    "with open('q_quora_100.csv', 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        questions.append(row[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first row of the csv, the column keys. For double checking that the columns are the ones we want to easily refer back to see which columns are which."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names\n",
      "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n"
     ]
    }
   ],
   "source": [
    "keys = questions[0]\n",
    "print 'column names'\n",
    "print keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify any columns that have the wrong number of columns. This was found because commas were used in some questions which messed up the csv parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in questions:\n",
    "    if len(row) != 6:\n",
    "        print 'WARNING: A COLUMN NEEDS FIXING' + str(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert python list to numpy array and delete the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '2'\n",
      " 'What is the step by step guide to invest in share market in india?'\n",
      " 'What is the step by step guide to invest in share market?' '0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "questions = np.array(questions)\n",
    "questions = np.delete(questions, 0, 0)\n",
    "print questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "First take a random entry in the dataset to get a feel for the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>46</td><td>93</td><td>94</td><td>How did Darth Vader fought Darth Maul in Star Wars Legends?</td><td>Does Quora have a character limit for profile descriptions?</td><td>0</td></tr><tr><td>46</td><td>93</td><td>94</td><td>How did Darth Vader fought Darth Maul in Star Wars Legends?</td><td>Does Quora have a character limit for profile descriptions?</td><td>0</td></tr><tr><td>46</td><td>93</td><td>94</td><td>How did Darth Vader fought Darth Maul in Star Wars Legends?</td><td>Does Quora have a character limit for profile descriptions?</td><td>0</td></tr><tr><td>46</td><td>93</td><td>94</td><td>How did Darth Vader fought Darth Maul in Star Wars Legends?</td><td>Does Quora have a character limit for profile descriptions?</td><td>0</td></tr><tr><td>46</td><td>93</td><td>94</td><td>How did Darth Vader fought Darth Maul in Star Wars Legends?</td><td>Does Quora have a character limit for profile descriptions?</td><td>0</td></tr><tr><td>46</td><td>93</td><td>94</td><td>How did Darth Vader fought Darth Maul in Star Wars Legends?</td><td>Does Quora have a character limit for profile descriptions?</td><td>0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "size = len(questions)\n",
    "random_index = random.randrange(size)\n",
    "random_question = questions[random_index]\n",
    "\n",
    "\n",
    "display(HTML(\n",
    "    '<table><tr>{}</tr></table>'.format(\n",
    "        '</tr><tr>'.join(\n",
    "            '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in random_question)) for row in random_question)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a count of how many duplicates and what percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates: 36 percentage: 0.349514563107%\n",
      "not duplicates: 67 percentage: 0.650485436893%\n"
     ]
    }
   ],
   "source": [
    "duplicates = 0\n",
    "not_duplicates = 0\n",
    "total = len(questions)\n",
    "\n",
    "for row in questions:\n",
    "    if row[5] == '0':\n",
    "        not_duplicates += 1\n",
    "    else:\n",
    "        duplicates += 1\n",
    "\n",
    "print 'duplicates: ' + str(duplicates) + ' percentage: ' + str((float(duplicates) / float(total))) + '%'\n",
    "print 'not duplicates: ' + str(not_duplicates) + ' percentage: ' + str(float(not_duplicates) / float(total)) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "### Munging\n",
    "\n",
    "Downcase and then split the sentence into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india?'], ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market?']], ['1', ['what', 'is', 'the', 'story', 'of', 'kohinoor', '(koh-i-noor)', 'diamond?'], ['what', 'would', 'happen', 'if', 'the', 'indian', 'government', 'stole', 'the', 'kohinoor', '(koh-i-noor)', 'diamond', 'back?']], ['2', ['how', 'can', 'i', 'increase', 'the', 'speed', 'of', 'my', 'internet', 'connection', 'while', 'using', 'a', 'vpn?'], ['how', 'can', 'internet', 'speed', 'be', 'increased', 'by', 'hacking', 'through', 'dns?']], ['3', ['why', 'am', 'i', 'mentally', 'very', 'lonely?', 'how', 'can', 'i', 'solve', 'it?'], ['find', 'the', 'remainder', 'when', '[math]23^{24}[/math]', 'is', 'divided', 'by', '24,23?']]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "processing = []\n",
    "\n",
    "\n",
    "for data in questions:\n",
    "    pairId = data[0]\n",
    "    sentence1 = data[3].lower()\n",
    "    sentence2 = data[4].lower()\n",
    "    matches = reg.match(data[3])\n",
    "    tokens1 = sentence1.split(' ')\n",
    "    tokens2 = sentence2.split(' ')\n",
    "    processing.append([\n",
    "        pairId,\n",
    "        tokens1,\n",
    "        tokens2\n",
    "    ])\n",
    "    \n",
    "print(processing[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', ['step', 'step', 'guide', 'invest', 'share', 'market', 'india'], ['step', 'step', 'guide', 'invest', 'share', 'market']], ['1', ['story', 'kohinoor', 'kohinoor', 'diamond'], ['would', 'happen', 'indian', 'government', 'stole', 'kohinoor', 'kohinoor', 'diamond', 'back']], ['2', ['increase', 'speed', 'internet', 'connection', 'using', 'vpn'], ['internet', 'speed', 'increased', 'hacking', 'dns']], ['3', ['mentally', 'lonely', 'solve', 'it'], ['find', 'remainder', 'mathmath', 'divided']], ['4', ['one', 'dissolve', 'water', 'quikly', 'sugar', 'salt', 'methane', 'carbon', 'di', 'oxide'], ['fish', 'would', 'survive', 'salt', 'water']]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexbeeken/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "munged = []\n",
    "\n",
    "def remove_stop(sentence, words=stopwords.words('english')):\n",
    "    remove_these = []\n",
    "    for i in range(0, len(sentence) - 1):\n",
    "        word = sentence[i]\n",
    "        if word in words:\n",
    "            remove_these.append(i)\n",
    "    output = []\n",
    "    for i in range(0, len(sentence)):\n",
    "        if i not in remove_these:\n",
    "            punctuationless = remove_punctuation(sentence[i])\n",
    "            if len(punctuationless) > 0:\n",
    "                output.append(punctuationless)\n",
    "    return output\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    return re.sub('[^a-zA-Z]', '', word)\n",
    "\n",
    "for data in processing:\n",
    "    pairId = data[0]\n",
    "    sentence1 = data[1]\n",
    "    sentence2 = data[2]\n",
    "    out1 = remove_stop(sentence1)\n",
    "    out2 = remove_stop(sentence2)\n",
    "        \n",
    "    munged.append([\n",
    "        pairId,\n",
    "        out1,\n",
    "        out2\n",
    "    ])\n",
    "    \n",
    "print munged[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Order words by popularity\n",
    "\n",
    "We'll create a list of all words used in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step', 'step', 'guide', 'invest', 'share', 'market', 'india', 'step', 'step', 'guide', 'invest', 'share', 'market', 'story', 'kohinoor', 'kohinoor', 'diamond', 'would', 'happen', 'indian']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "\n",
    "for row in munged:\n",
    "    for word in row[1]:\n",
    "        all_tokens.append(word)\n",
    "    for word in row[2]:\n",
    "        all_tokens.append(word)\n",
    "\n",
    "print all_tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll count the occurences of each word and order it by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('best', 15), ('make', 12), ('learn', 8), ('much', 8), ('like', 8), ('would', 7), ('find', 6), ('ever', 6), ('india', 6), ('someone', 6), ('book', 6), ('quora', 5), ('phone', 5), ('me', 5), ('good', 5), ('effects', 5), ('increase', 5), ('get', 5), ('ask', 5), ('one', 5)]\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_MAX = 1000\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "count = Counter(all_tokens)\n",
    "top_count = count.most_common(VOCABULARY_MAX)\n",
    "print top_count[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make the vocabulary count into a usable list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 words: ['best', 'make', 'learn', 'much', 'like', 'would', 'find', 'ever', 'india', 'someone', 'book', 'quora', 'phone', 'me', 'good', 'effects', 'increase', 'get', 'ask', 'one']\n",
      "\n",
      "actual vocabulary size: 676\n",
      "\n",
      "percentage of original corpus: 100.0%\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "\n",
    "for tup in top_count:\n",
    "    vocabulary.append(tup[0])\n",
    "\n",
    "print 'top 20 words: ' + str(vocabulary[0:20])\n",
    "print '\\nactual vocabulary size: ' + str(len(vocabulary))\n",
    "print '\\npercentage of original corpus: '+ str(float(len(vocabulary)) / float(len(count)) * 100) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the actual strings in our data to indexes in the vocabulary list. Examine how many words are left out of our vocabulary with our vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: \n",
      "['0', [43, 43, 217, 283, 124, 110, 8], [43, 43, 217, 283, 124, 110]]\n",
      "\n",
      "total none count: 15\n",
      "\n",
      "percentage of corpus removed: 1.30548302872%\n"
     ]
    }
   ],
   "source": [
    "indexes = []\n",
    "noneCount = 0\n",
    "\n",
    "def vocabIndex(token):\n",
    "    try:\n",
    "        return vocabulary.index(token)\n",
    "    except ValueError:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def indexify(tokens):\n",
    "    global noneCount\n",
    "    output = []\n",
    "    for token in tokens:\n",
    "        index = vocabIndex(token)\n",
    "        if index:\n",
    "            output.append(index)\n",
    "        else:\n",
    "            noneCount += 1\n",
    "    return output\n",
    "        \n",
    "for row in munged:\n",
    "    indexes.append([\n",
    "        row[0],\n",
    "        indexify(row[1]),\n",
    "        indexify(row[2])\n",
    "    ])\n",
    "\n",
    "print 'example: '\n",
    "print indexes[0]\n",
    "print '\\ntotal none count: ' + str(noneCount)\n",
    "print '\\npercentage of corpus removed: ' + str(float(noneCount) / float(len(all_tokens)) * 100) + '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
